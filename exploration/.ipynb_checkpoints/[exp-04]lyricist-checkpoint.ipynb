{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "visible-madrid",
   "metadata": {},
   "source": [
    "# 작사가 인공지능 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heated-constant",
   "metadata": {},
   "source": [
    "## 1. 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "owned-confusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "attached-attachment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['[Hook]', \"I've been down so long, it look like up to me\", 'They look up to me']\n"
     ]
    }
   ],
   "source": [
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담기.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "falling-cookbook",
   "metadata": {},
   "source": [
    "## 2. 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "decreased-editing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples:\n",
      " ['[Hook]', \"I've been down so long, it look like up to me\", 'They look up to me', \"I got fake people showin' fake love to me\", 'Straight up to my face, straight up to my face', \"I've been down so long, it look like up to me\", 'They look up to me', \"I got fake people showin' fake love to me\", 'Straight up to my face, straight up to my face [Verse 1]', \"Somethin' ain't right when we talkin'\", \"Somethin' ain't right when we talkin'\", \"Look like you hidin' your problems\", 'Really you never was solid', 'No, you can\\'t \"son\" me', \"You won't never get to run me\", 'Just when shit look out of reach', 'I reach back like one, three', 'Like one, three, yeah [Pre-Hook]', \"That's when they smile in my face\", 'Whole time they wanna take my place', 'Whole time they wanna take my place', 'Whole time they wanna take my place', 'Yeah, I know they wanna take my place', 'I can tell that love is fake', \"I don't trust a word you say\", 'How you wanna clique up after your mistakes?', \"Look you in the face, and it's just not the same [Hook]\", \"I've been down so long, it look like up to me\", 'They look up to me', \"I got fake people showin' fake love to me\", 'Straight up to my face, straight up to my face', \"I've been down so long, it look like up to me\", 'They look up to me', \"I got fake people showin' fake love to me\", 'Straight up to my face, straight up to my face [Verse 2]', 'Yeah, straight up to my face, tryna play it safe', 'Vibe switch like night and day', 'I can see it, like, right away', 'I came up, you changed up', 'I caught that whole play', 'Since, things never been the same [Pre-Hook]', \"That's when they smile in my face\", 'Whole time they wanna take my place', 'Whole time they wanna take my place', 'Whole time they wanna take my place', 'Yeah, I know they wanna take my place', 'I can tell that love is fake (I can tell that love is fake)', \"I don't trust a word you say (I don't trust a word)\", 'How you wanna clique up after your mistakes?', \"(That's just what I heard)\"]\n"
     ]
    }
   ],
   "source": [
    "#어떠한 데이터가 있는지 확인하기 위헤 \n",
    "print(\"Examples:\\n\", raw_corpus[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "solar-india",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() \n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) \n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) \n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) \n",
    "    sentence = sentence.strip() \n",
    "    sentence = '<start> ' + sentence + ' <end>' \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fancy-oxygen",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    # 우리가 원하지 않는 문장은 건너뜀.\n",
    "    if len(sentence) == 0:\n",
    "        continue\n",
    "    if len(sentence.split(' ')) > 12: # <start>, <end>를 붙이기 전의 문장이 12개 이하로 split 된다면 token도 14개 이하가 될 것.\n",
    "        continue\n",
    "    if sentence[-1] == ':':\n",
    "        continue\n",
    "    if sentence[0] == '[' and sentence[-1] == ']': # [hook]과 같은 형태 정제해줌\n",
    "        continue\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "original-brown",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157652\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-florida",
   "metadata": {},
   "source": [
    "### 2-2. tokenize화 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "mysterious-confidentiality",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=20000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    tensor = tokenizer.texts_to_sequences(corpus) # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen = 14)  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "controversial-generation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   5   91  103 ...   10   12    3]\n",
      " [   2   42  133 ...    0    0    0]\n",
      " [   2    5   40 ...    0    0    0]\n",
      " ...\n",
      " [   2  203    3 ...    0    0    0]\n",
      " [   2  437    9 ...    0    0    0]\n",
      " [   2    9 1613 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7fa6ff980cd0>\n"
     ]
    }
   ],
   "source": [
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruled-jaguar",
   "metadata": {},
   "source": [
    "# 3. 평가 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "intelligent-natural",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_input = tensor[:, :-1]  \n",
    "tgt_input = tensor[:, 1:]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "quick-australia",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_test_split사용해 분리\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "affiliated-camel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (126121, 13)\n",
      "Target Train: (126121, 13)\n",
      "Source Test: (31531, 13)\n",
      "Target Test: (31531, 13)\n"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)\n",
    "print(\"Source Test:\", enc_val.shape)\n",
    "print(\"Target Test:\", dec_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aquatic-processing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 13), (256, 13)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#하이퍼 파라미터 준비하기\n",
    "BUFFER_SIZE = len(enc_train)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(enc_train) // BATCH_SIZE\n",
    "\n",
    " # tokenizer가 구축한 단어사전 내 20000개와, 여기 포함되지 않은 0:<pad>를 포함하여 20001개\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "# 준비한 데이터 소스로부터 데이터셋 만들기\n",
    "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rising-forestry",
   "metadata": {},
   "source": [
    "# 4. 인공지능 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "demographic-rings",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        self.batchnorm_1 = tf.keras.layers.BatchNormalization()\n",
    "        self.batchnorm_2 = tf.keras.layers.BatchNormalization()\n",
    "        self.dropout = tf.keras.layers.Dropout(0.1)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.batchnorm_1(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.batchnorm_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "greenhouse-caribbean",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "quantitative-management",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "492/492 [==============================] - 189s 371ms/step - loss: 3.7667\n",
      "Epoch 2/10\n",
      "492/492 [==============================] - 183s 372ms/step - loss: 2.6920\n",
      "Epoch 3/10\n",
      "492/492 [==============================] - 183s 371ms/step - loss: 2.2421\n",
      "Epoch 4/10\n",
      "492/492 [==============================] - 183s 371ms/step - loss: 1.9110\n",
      "Epoch 5/10\n",
      "492/492 [==============================] - 183s 371ms/step - loss: 1.6749\n",
      "Epoch 6/10\n",
      "492/492 [==============================] - 183s 371ms/step - loss: 1.5047\n",
      "Epoch 7/10\n",
      "492/492 [==============================] - 182s 370ms/step - loss: 1.3824\n",
      "Epoch 8/10\n",
      "492/492 [==============================] - 183s 371ms/step - loss: 1.2887\n",
      "Epoch 9/10\n",
      "492/492 [==============================] - 183s 371ms/step - loss: 1.2169\n",
      "Epoch 10/10\n",
      "492/492 [==============================] - 183s 371ms/step - loss: 1.1645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa6ffb24510>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-beads",
   "metadata": {},
   "source": [
    "## 4-2. 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "involved-mediterranean",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    while True:\n",
    "        #입력받은 문장의 텐서 입력\n",
    "        predict = model(test_tensor)\n",
    "        # 예측된 값 중 가장 높은 확률의 word index 뽑음. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 위의 word index 문장에 붙힘. \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        #<end>나 maxlen에 도달하면 끝. \n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "likely-arrest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you <end> '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "working-specification",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> he s a monster <end> '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> he\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "earlier-vertical",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> love is a beautiful thing <end> '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> love\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "digital-robertson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> die while america drink your blood <end> '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> die\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "combined-assessment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> what you want nixga what you what you want nixga <end> '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> what\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-glass",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
