{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "first-playing",
   "metadata": {},
   "source": [
    "# 작사가 인공지능 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-produce",
   "metadata": {},
   "source": [
    "## 1. 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "boring-trust",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "equal-spectacular",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['[Hook]', \"I've been down so long, it look like up to me\", 'They look up to me']\n"
     ]
    }
   ],
   "source": [
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담기.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-parker",
   "metadata": {},
   "source": [
    "## 2. 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cubic-turkey",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples:\n",
      " ['[Hook]', \"I've been down so long, it look like up to me\", 'They look up to me', \"I got fake people showin' fake love to me\", 'Straight up to my face, straight up to my face', \"I've been down so long, it look like up to me\", 'They look up to me', \"I got fake people showin' fake love to me\", 'Straight up to my face, straight up to my face [Verse 1]', \"Somethin' ain't right when we talkin'\", \"Somethin' ain't right when we talkin'\", \"Look like you hidin' your problems\", 'Really you never was solid', 'No, you can\\'t \"son\" me', \"You won't never get to run me\", 'Just when shit look out of reach', 'I reach back like one, three', 'Like one, three, yeah [Pre-Hook]', \"That's when they smile in my face\", 'Whole time they wanna take my place', 'Whole time they wanna take my place', 'Whole time they wanna take my place', 'Yeah, I know they wanna take my place', 'I can tell that love is fake', \"I don't trust a word you say\", 'How you wanna clique up after your mistakes?', \"Look you in the face, and it's just not the same [Hook]\", \"I've been down so long, it look like up to me\", 'They look up to me', \"I got fake people showin' fake love to me\", 'Straight up to my face, straight up to my face', \"I've been down so long, it look like up to me\", 'They look up to me', \"I got fake people showin' fake love to me\", 'Straight up to my face, straight up to my face [Verse 2]', 'Yeah, straight up to my face, tryna play it safe', 'Vibe switch like night and day', 'I can see it, like, right away', 'I came up, you changed up', 'I caught that whole play', 'Since, things never been the same [Pre-Hook]', \"That's when they smile in my face\", 'Whole time they wanna take my place', 'Whole time they wanna take my place', 'Whole time they wanna take my place', 'Yeah, I know they wanna take my place', 'I can tell that love is fake (I can tell that love is fake)', \"I don't trust a word you say (I don't trust a word)\", 'How you wanna clique up after your mistakes?', \"(That's just what I heard)\"]\n"
     ]
    }
   ],
   "source": [
    "#어떠한 데이터가 있는지 확인하기 위헤 \n",
    "print(\"Examples:\\n\", raw_corpus[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "social-standard",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() \n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) \n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) \n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) \n",
    "    sentence = sentence.strip() \n",
    "    sentence = '<start> ' + sentence + ' <end>' \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "italian-vatican",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    # 우리가 원하지 않는 문장은 건너뜀.\n",
    "    if len(sentence) == 0:\n",
    "        continue\n",
    "    if len(sentence.split(' ')) > 12: # <start>, <end>를 붙이기 전의 문장이 12개 이하로 split 된다면 token도 14개 이하가 될 것.\n",
    "        continue\n",
    "    if sentence[-1] == ':':\n",
    "        continue\n",
    "    if sentence[0] == '[' and sentence[-1] == ']': # [hook]과 같은 형태 정제해줌\n",
    "        continue\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "intellectual-examination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157652\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-bosnia",
   "metadata": {},
   "source": [
    "### 2-2. tokenize화 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "intensive-feeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=20000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    tensor = tokenizer.texts_to_sequences(corpus) # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen = 14)  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "nutritional-praise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   5   91  103 ...   10   12    3]\n",
      " [   2   42  133 ...    0    0    0]\n",
      " [   2    5   40 ...    0    0    0]\n",
      " ...\n",
      " [   2  203    3 ...    0    0    0]\n",
      " [   2  437    9 ...    0    0    0]\n",
      " [   2    9 1613 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7fa6ff980cd0>\n"
     ]
    }
   ],
   "source": [
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-patient",
   "metadata": {},
   "source": [
    "# 3. 평가 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "engaging-championship",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_input = tensor[:, :-1]  \n",
    "tgt_input = tensor[:, 1:]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sensitive-fleet",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_test_split사용해 분리\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "changing-destruction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (126121, 13)\n",
      "Target Train: (126121, 13)\n",
      "Source Test: (31531, 13)\n",
      "Target Test: (31531, 13)\n"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)\n",
    "print(\"Source Test:\", enc_val.shape)\n",
    "print(\"Target Test:\", dec_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "respiratory-aside",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 13), (256, 13)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#하이퍼 파라미터 준비하기\n",
    "BUFFER_SIZE = len(enc_train)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(enc_train) // BATCH_SIZE\n",
    "\n",
    " # tokenizer가 구축한 단어사전 내 20000개와, 여기 포함되지 않은 0:<pad>를 포함하여 20001개\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "# 준비한 데이터 소스로부터 데이터셋 만들기\n",
    "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-weapon",
   "metadata": {},
   "source": [
    "# 4. 인공지능 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "demonstrated-evolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        self.batchnorm_1 = tf.keras.layers.BatchNormalization()\n",
    "        self.batchnorm_2 = tf.keras.layers.BatchNormalization()\n",
    "        self.dropout = tf.keras.layers.Dropout(0.1)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.batchnorm_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.batchnorm_2(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "covered-magic",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "placed-syria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "492/492 [==============================] - 188s 370ms/step - loss: 3.7574\n",
      "Epoch 2/10\n",
      "492/492 [==============================] - 184s 372ms/step - loss: 2.7304\n",
      "Epoch 3/10\n",
      "492/492 [==============================] - 184s 373ms/step - loss: 2.2865\n",
      "Epoch 4/10\n",
      "492/492 [==============================] - 183s 372ms/step - loss: 1.9801\n",
      "Epoch 5/10\n",
      "492/492 [==============================] - 183s 372ms/step - loss: 1.7531\n",
      "Epoch 6/10\n",
      "492/492 [==============================] - 183s 371ms/step - loss: 1.5808\n",
      "Epoch 7/10\n",
      "492/492 [==============================] - 183s 372ms/step - loss: 1.4520\n",
      "Epoch 8/10\n",
      "492/492 [==============================] - 183s 371ms/step - loss: 1.3540\n",
      "Epoch 9/10\n",
      "492/492 [==============================] - 184s 373ms/step - loss: 1.2754\n",
      "Epoch 10/10\n",
      "492/492 [==============================] - 184s 373ms/step - loss: 1.2151\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa6951e9090>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-modification",
   "metadata": {},
   "source": [
    "## 4-2. 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "scenic-armenia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    while True:\n",
    "        #입력받은 문장의 텐서 입력\n",
    "        predict = model(test_tensor)\n",
    "        # 예측된 값 중 가장 높은 확률의 word index 뽑음. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 위의 word index 문장에 붙힘. \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        #<end>나 maxlen에 도달하면 끝. \n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "talented-component",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you <end> '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "amber-gathering",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> he s a monster <end> '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> he\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "appropriate-helping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> love is a beautiful thing <end> '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> love\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "indian-melissa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> die while america drink your blood <end> '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> die\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "enormous-defense",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> what you want nixga what you what you want nixga <end> '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> what\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "excited-instrumentation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> he knows hes so fucking gifted <end> '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> he knows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-violin",
   "metadata": {},
   "source": [
    "## 5. 회고록"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-republic",
   "metadata": {},
   "source": [
    "1. loss값을 줄이는 것이 가장 관건인 과제였다.\n",
    "    - batch normalization\n",
    "    \n",
    "    - dropout\n",
    "    \n",
    "    위의 두 가지 방법을 사용해 loss 값을 2.2 아래로 줄일 수 있었다. \n",
    "\n",
    "2. dropout에 관한 회고\n",
    "\n",
    "    - 사실 dropout을 사용하기는 했지만, 정확히 어느 수준, 어느 곳에 사용해야 하는지에 대해 잘 알지 못했다. https://stats.stackexchange.com/questions/240305/where-should-i-place-dropout-layers-in-a-neural-network\n",
    "    위 글을 참고해 0.1수준으로 맞춰주었다. output 이전에 위치해주는 것이 좋다고 한다. \n",
    "    \n",
    "3. Batch Normalization에 관한 회고\n",
    "    \n",
    "    - input normalization의 방법 중에 하나이다. scaling 효과를 얻기 위해 써주었고, after fully connected or\n",
    "    convolutional layers / before nonlinearity의 위치에 넣어주었다. \n",
    "    \n",
    "4. 자연어에 대한 프로젝트는 처음이라 굉장히 어려웠다. 특히 tokennize의 개념이 맨 처음에 굉장히 어려웠다. 하지만, 천천히 따라가다 보니 어느정도\n",
    "    따라갈 수 있었다. 하지만, 모델의 성능을 높이는 부문이 굉장히 어려웠다. \n",
    "    딥러닝의 개념을 배우고 있긴 하지만, 그것을 실질적으로 써보는 경험이 부족해 이론과 구현의 괴리가 심하다. \n",
    "    또한, 이렇게 큰 데이터 셋은 한번 돌리면 시간이 굉장히 많이 걸리는데, dropout의 비율이나 위치등의 hyperparameter handling 시에 \n",
    "    모든 스텝을 다 일일히 트레이닝 시켜보고 기다려야 하는 것인지, 아니면 내가 모르는 방법이 있는 것인지 궁금하다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adequate-context",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
